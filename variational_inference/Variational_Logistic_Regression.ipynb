{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "iEJNha5PvMdi"
      },
      "source": [
        "# Advanced Statistical Inference -- Variational Inference for Bayesian Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "wz3BGUBKvMdl"
      },
      "source": [
        "In this notebook, you will learn how to implement the (stochastic) variational inference algorithm.\n",
        "The gist below will serve you as a refresh on variational inference. \n",
        "For additional information, please check the lecture notes. \n",
        "\n",
        "In the general setting, given a probabilistic model with observations $\\{\\boldsymbol{X},\\boldsymbol{y}\\}$, model parameters $\\boldsymbol{w}$ and likelihood $p(\\boldsymbol{y}|\\boldsymbol{X}, \\boldsymbol{w})$, by introducing an approximate posterior distribution $q_\\theta(\\boldsymbol{w})$ with parameters $\\theta$, the variational lower bound to the log-marginal likelihood is defined as\n",
        "\n",
        "\n",
        "$$\\mathrm{KL}[{q_\\theta(\\boldsymbol{w})}||{p(\\boldsymbol{w}|\\boldsymbol{X},\\boldsymbol{y})}] = -\\mathbb{E}_{q_{\\theta}}\\log p(\\boldsymbol{y}|\\boldsymbol{X}, \\boldsymbol{w}) +  \\mathrm{KL}[{q_{\\theta}(\\boldsymbol{w})}||{p(\\boldsymbol{w})}] + \\log p(\\boldsymbol{y}|\\boldsymbol{X}) $$\n",
        "\n",
        "\n",
        "The objective is then to maximize this variational bound (or evidence lower bound):\n",
        "\n",
        "$$\n",
        "      \\mathcal{L}(\\theta) = \\underbrace{\\mathbb{E}_{q_{\\theta}}\\log p(\\boldsymbol{y}|\\boldsymbol{X}, \\boldsymbol{w})}_\\text{Expected loglikelihood} -  \\mathrm{KL}[{q_{\\theta}(\\boldsymbol{w})}||{p(\\boldsymbol{w})}]\n",
        "$$\n",
        "\n",
        "The analytic evaluation of the ELBO is generally still untractable due to the presence of the expected loglikelihood under the variational distribution (in the majority of  cases the rightmost KL is tractable).\n",
        "This is commonly overcome by sampling $N_\\mathrm{MC}$ times from $q_\\theta$  using the reparameterization trick\n",
        "\n",
        "$$\n",
        "    \\mathbb{E}_{q_{\\theta}}\\log p(\\boldsymbol{y}|\\boldsymbol{X}, \\boldsymbol{w}) \\approx \\dfrac{1}{N_\\mathrm{MC}} \\sum_{\\tilde{\\boldsymbol{w}}_i\\sim q_\\theta} \\log p(\\boldsymbol{y}|\\boldsymbol{X}, \\tilde{\\boldsymbol{w}}_i)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3ZKKHH3vMdp"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import rc\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "colab = \"google.colab\" in str(get_ipython())\n",
        "preamble = r\"\"\"\\renewcommand{\\familydefault}{\\sfdefault}\\usepackage{sansmath}\n",
        "\\usepackage{FiraSans}\\sansmath\\usepackage{amsmath}\"\"\"\n",
        "\n",
        "rc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": \"DejaVu Sans\"})\n",
        "rc(\"text\", **{\"usetex\": not colab, \"latex.preamble\": preamble})\n",
        "rc(\"figure\", **{\"dpi\": 200})\n",
        "rc(\n",
        "    \"axes\",\n",
        "    **{\"spines.right\": False, \"spines.top\": False, \"xmargin\": 0.0, \"ymargin\": 0.05}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "LbdM_r0ZvMdr"
      },
      "source": [
        "## Library and coding\n",
        "\n",
        "This lab is heavily built on JAX. You already used JAX during the lab on Gaussian process regression when you took advantage of the automatic differentation engine to optimize the kernel parameters, without manually writing the derivatives by hand. \n",
        "This time, we will this same auto-diff engine to optimize our variational objective, but we will also take advantage of another key charateristic of JAX: automatic vectorization. This will allow us to efficiently parallelize the computation of the expected likelihood. \n",
        "Below, you have a short tutorial on how this works.\n",
        "\n",
        "Consider the following simple code that computes the convolution of two one-dimensional vectors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5DQDBYOvMdt"
      },
      "outputs": [],
      "source": [
        "x = jnp.array([0.0, 1.0, 2.0, 3.0, 4.0])\n",
        "w = jnp.array([2.0, 3.0, 4.0])\n",
        "\n",
        "\n",
        "def convolve(x, w):\n",
        "    y = []\n",
        "    for i in range(1, len(x) - 1):\n",
        "        y.append(jnp.dot(x[i - 1 : i + 2], w))\n",
        "    return jnp.array(y)\n",
        "\n",
        "\n",
        "y = convolve(x, w)\n",
        "print(f\"x = {x}\\nw = {w}\\ny = {y}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHKMJJ06vMdu"
      },
      "source": [
        "Suppose we would like to apply this function to a batch of weights `w` to a batch of vectors `x`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt7WpV1rvMdv"
      },
      "outputs": [],
      "source": [
        "X = jnp.stack([x, 2 * x])\n",
        "W = jnp.stack([w, 2 * w])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA-jiqjMvMdw"
      },
      "source": [
        "The most naive option would be to simply loop over the batch in Python and then stack the results; this produces the correct result, however it is not very efficient.\n",
        "In order to batch the computation efficiently, you would normally have to rewrite the function manually to ensure it is done in vectorized form. \n",
        "This is not particularly difficult to implement for this particular example, but does involve changing how the function treats indices, axes, and other parts of the input.\n",
        "Such re-implementation can become messy to understand and error-prone; fortunately JAX provides another way.\n",
        "In JAX, the `jax.vmap` transformation is designed to generate such a vectorized implementation of a function automatically:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPUO0hscvMdx"
      },
      "outputs": [],
      "source": [
        "Y = jax.vmap(convolve)(X, W)\n",
        "print(f\"X = \\n{X}\\nW =  \\n{W}\\nY =  \\n{Y}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDvrDBcqvMdy"
      },
      "source": [
        "Additional you can control what to vectorize using `in_axes=[...]`. For example, if you would like to convolve to a single set of weights `w` with a batch of vectors `X` you need to choose `in_axes=[0, None]`: this means \"vectorize on the axis 0 of the first arg but do *not* vectorize on the second arg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiUdInUVvMdz"
      },
      "outputs": [],
      "source": [
        "Y = jax.vmap(convolve, in_axes=[0, None])(X, w)\n",
        "print(f\"X = \\n{X}\\nw = \\n{w}\\nY =  \\n{Y}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKA3m7QlvMd0"
      },
      "source": [
        "Finally, this `vmap` can be also used as decorator, thus keeping the code cleaner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "cUOwhm7rvMd0"
      },
      "outputs": [],
      "source": [
        "@partial(jax.vmap, in_axes=[0, None])\n",
        "def convolve(x, w):\n",
        "    y = []\n",
        "    for i in range(1, len(x) - 1):\n",
        "        y.append(jnp.dot(x[i - 1 : i + 2], w))\n",
        "    return jnp.array(y)\n",
        "\n",
        "\n",
        "Y = convolve(X, w)\n",
        "print(f\"X = \\n{X}\\nw =\\n {w}\\nY =  \\n{Y}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "QJ_zEEblvMd1"
      },
      "source": [
        "### Random number generation in JAX \n",
        "To fully exploit the automatic vectorization of JAX, we will need to switch from the Numpy's random generation to JAX's. \n",
        "The main difference is that JAX is explicit on the definition and usage of the random seed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIQ4Ydj9vMd1"
      },
      "outputs": [],
      "source": [
        "rng = jax.random.PRNGKey(0)\n",
        "print(jax.random.uniform(rng))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCR15Q6UvMd2"
      },
      "source": [
        "This also means that in order to have \"true\" random number generation, we need to manually advance the random seed. **The rule of thumb is: never reuse keys (unless you want identical outputs).**\n",
        "In order to generate different and independent samples, you must `split()` the key *yourself* whenever you want to call a random function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBT3pTThvMd2"
      },
      "outputs": [],
      "source": [
        "print(\"rng\", rng, \"--> uniform\", jax.random.uniform(rng))\n",
        "rng, new_rng = jax.random.split(rng)\n",
        "normal_sample = jax.random.uniform(new_rng)\n",
        "print(r\"    \\---SPLIT --> new_rng\", new_rng, \"--> uniform\", jax.random.uniform(new_rng))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ48liUgvMd3"
      },
      "source": [
        "While all this sounds exceedingly over-complicated, it also allows to very easily vectorize using `jax.vmap` functions that works on random samples. Take a look at the next cell and try to understand what is going on here. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRiIZgeqvMd3"
      },
      "outputs": [],
      "source": [
        "def take_square(rng):\n",
        "    x = jax.random.uniform(rng)\n",
        "    return x**2\n",
        "\n",
        "\n",
        "multiple_rng = jax.random.split(rng, 10)\n",
        "print(jax.vmap(take_square)(multiple_rng))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O63Z2vgOvMd4"
      },
      "source": [
        "# 1. Setup and data\n",
        "\n",
        "Similarly to the previous lab, youâ€™re going to implement the VI algorithm described in the lecture for binary classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "NuWvSwfgvMd4"
      },
      "outputs": [],
      "source": [
        "def plot_data(X, y, ax):\n",
        "    mask = y == 1\n",
        "    config = dict(edgecolor=\"black\", linewidth=1, zorder=10)\n",
        "    ax.scatter(*X[mask].T, label=\"Class 1\", facecolor=\"tab:blue\", **config)\n",
        "    ax.scatter(*X[~mask].T, label=\"Class 0\", facecolor=\"tab:orange\", **config)\n",
        "\n",
        "\n",
        "def get_grid(xlim=(-3, 3), ylim=None, N=100):\n",
        "    if ylim is None:\n",
        "        ylim = xlim\n",
        "    x_grid = np.linspace(*xlim, N)\n",
        "    y_grid = np.linspace(*ylim, N)\n",
        "    xx, yy = np.meshgrid(x_grid, y_grid)\n",
        "    X_plot = np.vstack((xx.flatten(), yy.flatten())).T\n",
        "    return xx, yy, X_plot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/srossi93/asi-labs/master/lab_week4-public/binaryclass2.csv"
      ],
      "metadata": {
        "id": "_PrlezKgvOxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "VDlEiSQevMd5"
      },
      "outputs": [],
      "source": [
        "data = np.loadtxt(\"binaryclass2.csv\", delimiter=\",\")\n",
        "X = data[..., :-1]\n",
        "y = data[..., -1]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=[5, 3])\n",
        "plot_data(X, y, ax)\n",
        "ax.set_title(\"Binary classification\")\n",
        "ax.legend()\n",
        "ax.set_xlim(-6, 6), ax.set_ylim(-6, 6)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ8u70xsvMd5"
      },
      "source": [
        "Let's start by defining some probability distributions that we will need in this notebook. \n",
        "First, the Bernoulli distribution. \n",
        "\n",
        "**Exercise:**\n",
        "Complete the following class to compute the logdensity of the Bernoulli distribution. Since we will be taking gradients of this likelihood, it's possible that during optimization $p$ becomes (down to machine precision) exactly 0 or 1, which makes the evaluation of the logarithm troublesome. To avoide this behavior, simply add a small quantity (like `1e-10`) before computing the log."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1LcAwbSvMd6"
      },
      "outputs": [],
      "source": [
        "def bernoulli_density(y, p):\n",
        "    return # @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y-GWwp4vMd6"
      },
      "source": [
        "**Exercise:** \n",
        "Now let's move to the Gaussian distribution. Note that the parameter $\\sigma^2$ is always expected to be positive while it is possible that the optimisation algorithm attempts to evaluate the log-likelihood in regions of the parameter space where one or more of these parameters are negative, leading to numerical issues. \n",
        "A commonly-used technique to enforce this condition is to work with a transformed version of parameters using the logarithm transformation. \n",
        "In particular, define $\\psi = \\log\\sigma^2$. \n",
        "So remember to take the exponential of $\\psi$ if you want to use $\\sigma^2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6tEhTaFvMd7"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from typing import NamedTuple\n",
        "\n",
        "\n",
        "class GaussianDiagonal(NamedTuple):\n",
        "    mean: jnp.array\n",
        "    log_var: jnp.array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syVPqsDvvMd7"
      },
      "source": [
        "**Exercise**: Complete the next function to sample once using the reparameterization trick. Remember $p(\\boldsymbol z) = p(t(\\boldsymbol \\varepsilon;\\boldsymbol \\theta))$ where $t(\\cdot) = \\boldsymbol \\mu + \\boldsymbol \\sigma \\odot \\boldsymbol \\varepsilon$ and $\\boldsymbol \\varepsilon \\sim \\mathcal{N}(\\boldsymbol 0, \\boldsymbol 1)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxhPJgqLvMd8"
      },
      "outputs": [],
      "source": [
        "def sample_gaussian_diagonal(rng, params):\n",
        "    mean, log_var = params\n",
        "    eps = # @@ COMPLETE @@ #\n",
        "    return # @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxv7NSFGvMd8"
      },
      "source": [
        "**Exercise**: Create a Gaussian with two components with $\\mathcal{N}(\\boldsymbol 0, \\boldsymbol 1)$ and get one sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIa-lfEvvMd8"
      },
      "outputs": [],
      "source": [
        "rng = jax.random.PRNGKey(0)\n",
        "p_params = # @@ COMPLETE @@ #\n",
        "# @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c3nrQu8vMd9"
      },
      "source": [
        "# 2. KL Divergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8w55x7PvMd9"
      },
      "source": [
        "The expression of the KL divergence between multivariate Gaussians $q = \\mathcal{N}(\\boldsymbol{\\mu}_q, \\boldsymbol\\Sigma_q)$ and $p = \\mathcal{N}(\\boldsymbol{\\mu}_p, \\boldsymbol\\Sigma_p)$ is as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathrm{KL}[q || p] = \n",
        "\\frac{1}{2} \\mathrm{Tr}(\\boldsymbol\\Sigma_p^{-1} \\boldsymbol\\Sigma_q)\n",
        "+ \\frac{1}{2} (\\boldsymbol\\mu_p - \\boldsymbol\\mu_q)^{\\top} \\boldsymbol\\Sigma_1^{-1} (\\boldsymbol\\mu_p - \\boldsymbol\\mu_q)\n",
        "- \\frac{D}{2} \n",
        "+ \\frac{1}{2} \\log\\left( \\frac{\\mathrm{det}\\boldsymbol\\Sigma_p}{\\mathrm{det}\\boldsymbol\\Sigma_q} \\right)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "This formula simplifies when the two Gaussians have diagonal covariance, i.e. $q = \\mathcal{N}(\\boldsymbol{\\mu}_q, \\boldsymbol{\\sigma}^2_q\\mathrm{I})$ and $p = \\mathcal{N}(\\boldsymbol{\\mu}_p, \\boldsymbol{\\sigma}^2_p\\mathrm{I})$,\n",
        "\n",
        "$$\n",
        "\\mathrm{KL}[q || p] =  \\frac{1}{2} \\sum\\left( \\log \\frac{\\sigma^2_p}{\\sigma^2_q} + \\frac{\\sigma_q^2 + (\\mu_q - \\mu_p)^2}{\\sigma_p^2} - 1 \\right)\n",
        "$$\n",
        "\n",
        "\n",
        "**Exercise:** \n",
        "Complete the next function to compute the KL divergence between two multivariate Gaussian distribution with diagonal covariance. *Note:* Since we have parameterized the Gaussian distribution with the logvariance, the formula above can be simplified even further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kayty4yuvMd9"
      },
      "outputs": [],
      "source": [
        "def kl_diag_diag(q_params, p_params):\n",
        "    assert isinstance(q_params, GaussianDiagonal)\n",
        "    assert isinstance(p_params, GaussianDiagonal)\n",
        "    mean_q, log_var_q = q_params\n",
        "    mean_p, log_var_p = p_params\n",
        "    # @@ COMPLETE @@ #\n",
        "    return kl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52e1xtN6vMd-"
      },
      "source": [
        "**Exercise:**\n",
        "Create two identical Gaussian distributions and compute the KL divergence using the function. What's the result? Is it what you were expecting? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyZDYVONvMd-"
      },
      "outputs": [],
      "source": [
        "q_params = # @@ COMPLETE @@ #\n",
        "\n",
        "print(\"KL =\", kl_diag_diag(q_params, p_params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts-u6N4cvMd_"
      },
      "source": [
        "# 3. Model\n",
        "\n",
        "Now we can move to design the model. We will use a simple logistic regression very similarly to what done in the previous MCMC lab.\n",
        "This very simple model computes $h(\\boldsymbol{x}) = h(\\boldsymbol{w}^\\top\\boldsymbol{x})$, where $h(\\cdot)$ is the logistic function. \n",
        "\n",
        "**Exercise:** \n",
        "Complete the next two functions to compute (i) the logistic function and (ii) the output of the model, given a particular choice of $\\boldsymbol w$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVWt1MsZvMd_"
      },
      "outputs": [],
      "source": [
        "def logistic(z):\n",
        "    return # @@ COMPLETE @@ #\n",
        "\n",
        "\n",
        "def model(w, X):\n",
        "    return # @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "-eWSZU6bvMeA"
      },
      "source": [
        "# 4. Variational objective\n",
        "\n",
        "The objective is to maximize this variational bound:\n",
        "$$\n",
        "      \\mathcal{L}(\\theta) = \\underbrace{\\mathbb{E}_{q_{\\theta}}\\log p(\\boldsymbol{y}|\\boldsymbol{X}, \\boldsymbol{w})}_\\text{Expected loglikelihood} -\\mathrm{KL}[{q_{\\theta}(\\boldsymbol{w})}||{p(\\boldsymbol{w})}]\n",
        "$$\n",
        "\n",
        "**Exercise:**\n",
        "Complete the next cell to compute the ELBO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmqtQMsNvMeA"
      },
      "outputs": [],
      "source": [
        "def create_elbo_fn(sample_fn, likelihood_fn, kl_divergence_fn):\n",
        "    \"\"\"Create a function to compute the ELBO, given the function to sample\n",
        "    from the posterior, the likelihood function and the KL divergence\n",
        "    \"\"\"\n",
        "\n",
        "    @partial(jax.vmap, in_axes=[0, None, None, None])\n",
        "    def likelihood_sample_fn(rng, q_params, X, y):\n",
        "        \"\"\"Compute the likelihood with one Monte Carlo sample of the posterior\n",
        "        The function is decorated to vectorize multiple MC samples automatically\n",
        "        \"\"\"\n",
        "        # Get one sample of w using the sample_fn and the parameters of q\n",
        "        w = # @@ COMPLETE @@ #\n",
        "        # Predict the output using the sample before\n",
        "        yp = # @@ COMPLETE @@ #\n",
        "        # Compute the likelihood and return it\n",
        "        return # @@ COMPLETE @@ #\n",
        "\n",
        "    def elbo_fn(q_params, p_params, rng, X, y, Nmc=1):\n",
        "        \"\"\"Computes the ELBO with multiple samples\"\"\"\n",
        "        # Split the random seed in Nmc times\n",
        "        rng = jax.random.split(rng, Nmc)\n",
        "\n",
        "        # Compute the values of the likelihood\n",
        "        likelihood_vals = # @@ COMPLETE @@ #\n",
        "        # Compute the expectation (i.e. take the mean)\n",
        "        expected_likelihood = # @@ COMPLETE @@ #\n",
        "        # Compute the KL divergence\n",
        "        kl = # @@ COMPLETE @@ #\n",
        "        # Compute the ELBO\n",
        "        elbo = # @@ COMPLETE @@ #\n",
        "        # Return the ELBO and its two term (used later for logging)\n",
        "        return elbo, (expected_likelihood, kl)\n",
        "\n",
        "    return elbo_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH45xHtmvMeB"
      },
      "source": [
        "**Exercise:**\n",
        "Using the function above, create the function to compute the ELBO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnqnyUJ2vMeB"
      },
      "outputs": [],
      "source": [
        "elbo_fn = create_elbo_fn(\n",
        "    sample_fn=# @@ COMPLETE @@ #\n",
        "    kl_divergence_fn=# @@ COMPLETE @@ #\n",
        "    likelihood_fn=# @@ COMPLETE @@ #\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DHqy2FovMeB"
      },
      "source": [
        "**Exercise:**\n",
        "Try to compute the variational objective (use 10 Monte Carlo samples)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHHyBKoVvMeC"
      },
      "outputs": [],
      "source": [
        "elbo, (likelihood, kl) = # @@ COMPLETE @@ #\n",
        "print(\"ELBO =\", elbo)\n",
        "print(\"Likelihood =\", likelihood)\n",
        "print(\"KL =\", kl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRes3SovvMeC"
      },
      "source": [
        "## 4.1 Analysis of the MC estimate of the ELBO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwDg6oeOvMeC"
      },
      "source": [
        "Ok, now that everything is done and ready we can start to make some analysis.\n",
        "\n",
        "First of all, as we said we don't have an analytical formula for the variational objective (our loss). We can only access (unbiased) samples, hence the next question.\n",
        "\n",
        "**Exercise:**\n",
        "Try to sample using 100 different random seeds the ELBO with [2, 10, 100, 1000] MC samples and plot their distribution with boxplots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRIqiCDOvMeC"
      },
      "outputs": [],
      "source": [
        "elbo_samples = pd.DataFrame()\n",
        "n_repetition = 100\n",
        "\n",
        "for Nmc in [2, 10, 100, 1000]:\n",
        "    elbo_samples[Nmc] = np.stack(\n",
        "        [\n",
        "            elbo_fn(q_params, p_params, jax.random.PRNGKey(i), X, y, Nmc=Nmc)[0]\n",
        "            for i in range(n_repetition)\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl43r_MOvMeD"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=[8, 4])\n",
        "sns.boxplot(data=elbo_samples, whis=np.inf)\n",
        "ax.set_title(\"Samples of the MC estimate of the ELBO\")\n",
        "ax.set_xlabel(\"Number of MC samples\")\n",
        "ax.set_ylabel(\"ELBO\")\n",
        "ax.margins(0, 0.05)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDI4wp3DvMeD"
      },
      "source": [
        "We said that, in case of large datasets, this can be computationally challenging, due to the evaluation of the likelihood $N_\\mathrm{MC}$ times.\n",
        "But we know that the ELBO can be approximated even further using mini-batching.\n",
        "Taking a random subset of data $\\mathcal{B}$, the approximation becomes\n",
        "\n",
        "$$\n",
        "    \\mathbb{E}_{q_{\\theta}}\\log p(\\boldsymbol{y}|\\boldsymbol{X}, \\boldsymbol{w}) \\approx \\dfrac{1}{N_\\mathrm{MC}} \\frac{N}{|\\mathcal{B}|}\\sum_{\\tilde{\\boldsymbol{w}}_i\\sim q_\\theta} \\sum_{\\boldsymbol{X}_j, \\boldsymbol{y}_j\\sim\\mathcal{B}} \\log p(\\boldsymbol{y}_j|\\boldsymbol{X}_j, \\tilde{\\boldsymbol{w}}_i)\n",
        "$$\n",
        "\n",
        "This introduces even more variance in the estimate of the ELBO but it allows to scale to (virtually) any sized dataset.\n",
        "You are free to extend this to our ELBO estimator, but for the simple example we are using today, this is not required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hw-RzYhvMeE"
      },
      "source": [
        "# 5. Optimization\n",
        "\n",
        "Ok, now we can move to the optimization of the ELBO: \n",
        "$$\n",
        "\\boldsymbol \\theta_{t+1} = \\boldsymbol \\theta_t + \\text{lr}\\cdot(\\nabla_{\\boldsymbol \\theta} \\mathcal{L})(\\boldsymbol \\theta_t)\n",
        "$$\n",
        "Below you have a simple function to implement this update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GguSZWMvvMeE"
      },
      "outputs": [],
      "source": [
        "def sg_update(params, gradients, learning_rate=1e-3):\n",
        "    updated_params = jax.tree_map(lambda p, g: p + learning_rate * g, params, gradients)\n",
        "    return updated_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JFLoGIDvMeF"
      },
      "source": [
        "And here the function to compute the gradient (this function will also be compiled using `jax.jit` for better optimization and faster running times)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk2IU1XmvMeF"
      },
      "outputs": [],
      "source": [
        "grad_elbo_fn = jax.grad(elbo_fn, has_aux=True)\n",
        "grad_elbo_fn = jax.jit(grad_elbo_fn, static_argnames=(\"Nmc\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmQVXB9DvMeF"
      },
      "source": [
        "**Exercise:**\n",
        "Write the training loop to optimize the ELBO (use a learning rate of $10^{-3}$). At every step, store the value of the ELBO, of the expected likelihood and the KL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZxjJ53pvMeG"
      },
      "outputs": [],
      "source": [
        "rng = jax.random.PRNGKey(0)\n",
        "q_params = GaussianDiagonal(jnp.zeros(2), jnp.zeros(2))\n",
        "\n",
        "elbo_summary = []\n",
        "lik_summary = []\n",
        "kl_summary = []\n",
        "\n",
        "for i in tqdm(range(10000), desc=\"Training ELBO\"):\n",
        "    rng, rng2 = jax.random.split(rng)\n",
        "    q_params_grad, (likelihood, kl) = # @@ COMPLETE @@ #\n",
        "    q_params = # @@ COMPLETE @@ #\n",
        "\n",
        "    lik_summary.append(likelihood)\n",
        "    kl_summary.append(kl)\n",
        "    elbo_summary.append(likelihood - kl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is7ih1_bvMeG"
      },
      "outputs": [],
      "source": [
        "print(\"Converged posterior\")\n",
        "print(\"Mean =\", q_params.mean)\n",
        "print(\"Var =\", np.exp(q_params.log_var))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ4PZAGLvMeH"
      },
      "source": [
        "**Exercise:**\n",
        "Assess convergence of the optimization by plotting the three metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-btA1QxvMeH"
      },
      "outputs": [],
      "source": [
        "fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=[8, 2.5])\n",
        "\n",
        "ax0.plot(elbo_summary, label=\"ELBO\")\n",
        "ax1.plot(\n",
        "    lik_summary,\n",
        "    color=\"tab:orange\",\n",
        "    label=r\"$E_{q(\\mathbf{w})} \\log p(\\mathbf{y}|\\mathbf{X},\\mathbf{w})$\",\n",
        ")\n",
        "ax2.plot(\n",
        "    kl_summary,\n",
        "    color=\"tab:green\",\n",
        "    label=r\"$\\mathrm{KL}[{q(\\mathbf{w})}||{p(\\mathbf{w})}]$\",\n",
        ")\n",
        "\n",
        "ax0.semilogx()\n",
        "ax1.semilogx()\n",
        "ax2.semilogx()\n",
        "\n",
        "ax0.axhline(-0.4, color=\"xkcd:red\", label=r\"$\\log p(\\mathbf{y}|\\mathbf{X})$\")\n",
        "ax0.legend(bbox_to_anchor=(1.05, -0.05), loc=\"lower right\")\n",
        "ax1.legend(bbox_to_anchor=(1.05, -0.05), loc=\"lower right\")\n",
        "ax2.legend(bbox_to_anchor=(1.05, -0.05), loc=\"lower right\")\n",
        "\n",
        "ax0.set_xlabel(\"Iteration\")\n",
        "ax1.set_xlabel(\"Iteration\")\n",
        "ax2.set_xlabel(\"Iteration\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNf8ltQavMeH"
      },
      "source": [
        "**Question:**\n",
        "Analyze the behaviour of these three values and comment the plots. Focus you analysis on the breakdown of the ELBO in its parts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGg9vOaovMeI"
      },
      "source": [
        "Now the predictions! \n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbb{E}_{q(\\boldsymbol{w})}h(\\boldsymbol{w}^\\top\\boldsymbol{x}_\\mathrm{new}) = \\int h(\\boldsymbol{w}^\\top\\boldsymbol{x}_\\mathrm{new}) q(\\boldsymbol{w}) \\mathrm{d}\\boldsymbol{w}\n",
        "\\end{equation}\n",
        "\n",
        "**Exercise**:\n",
        "With 1000 samples, compute the probability $P (y_\\mathrm{new} = 1 | \\boldsymbol{x}_\\mathrm{new}, \\boldsymbol{X}, \\boldsymbol{y})$ when $\\boldsymbol{x}_\\mathrm{new} = [2,-4]^\\top$. Compare the result with the number you got from the previous lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pedu_08jvMeI"
      },
      "outputs": [],
      "source": [
        "def predict_y(sample_fn, q_params, Xt, rng, Nmc=10):\n",
        "    \"\"\"Compute the outputs of the model by sampling the posterior,\n",
        "    then take the expectation\n",
        "    \"\"\"\n",
        "\n",
        "    def predict_y_single(rng):\n",
        "        w = # @@ COMPLETE @@ #\n",
        "        yp = # @@ COMPLETE @@ #\n",
        "        return yp\n",
        "\n",
        "    rng = jax.random.split(rng, Nmc)\n",
        "    Xt = jnp.atleast_1d(Xt)\n",
        "    yp = jax.vmap(predict_y_single)(rng)\n",
        "    # Take the mean\n",
        "    yp = # @@ COMPLETE @@ #\n",
        "    return yp\n",
        "\n",
        "\n",
        "yp = predict_y(sample_gaussian_diagonal, q_params, [2, -4], rng, Nmc=1000)\n",
        "print(yp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6boHjWWqvMeI"
      },
      "source": [
        "**Execise:**\n",
        "Let's now plot the distribution at convergence and the predictions on a grid of points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rM9o3R-vMeJ"
      },
      "outputs": [],
      "source": [
        "import scipy.stats\n",
        "\n",
        "\n",
        "def plot_gaussian(params, **kwargs):\n",
        "    ax = kwargs.pop(\"ax\", plt.gca())\n",
        "    xx, yy, w_plot = get_grid(xlim=(-1.5, 5.5), N=250)\n",
        "    if isinstance(params, GaussianDiagonal):\n",
        "        cov = np.exp(params.log_var) * np.eye(2)\n",
        "    else:\n",
        "        cov = params.L @ params.L.T\n",
        "    zz = (\n",
        "        scipy.stats.multivariate_normal(mean=params.mean, cov=cov)\n",
        "        .pdf(w_plot)\n",
        "        .reshape(*xx.shape)\n",
        "    )\n",
        "\n",
        "    levels = np.linspace(1e-5, np.max(zz), 10)\n",
        "    ax.contourf(xx, yy, zz, cmap=\"cividis\", alpha=0.8, levels=levels)\n",
        "    ax.contour(xx, yy, zz, cmap=\"cividis\", levels=levels)\n",
        "\n",
        "    ax.set_xlabel(r\"$\\boldsymbol{w}_0$\")\n",
        "    ax.set_ylabel(r\"$\\boldsymbol{w}_1$\")\n",
        "\n",
        "\n",
        "def plot_posterior(ax):\n",
        "    xx, yy, w_plot = get_grid(xlim=(-1.5, 5.5), N=250)\n",
        "    zz = np.zeros(len(w_plot))\n",
        "    for i, w in enumerate(w_plot):\n",
        "        zz[i] = bernoulli_density(y, model(w, X)).sum() -0.5* w.T@w\n",
        "    zz = np.exp(zz).reshape(*xx.shape)\n",
        "    levels = np.linspace(1e-5, np.max(zz), 10)\n",
        "    ax.contourf(xx, yy, zz, cmap=\"cividis\", alpha=0.8, levels=levels)\n",
        "    ax.contour(xx, yy, zz, cmap=\"cividis\", levels=levels)\n",
        "    ax.set_xlabel(r\"$\\boldsymbol{w}_0$\")\n",
        "    ax.set_ylabel(r\"$\\boldsymbol{w}_1$\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV4sKQOzvMeJ"
      },
      "outputs": [],
      "source": [
        "fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=[12, 4])\n",
        "\n",
        "plot_gaussian(p_params, ax=ax0)\n",
        "plot_posterior(ax=ax1)\n",
        "plot_gaussian(q_params, ax=ax2)\n",
        "\n",
        "ax0.set_title(\"Prior\")\n",
        "ax1.set_title(\"True posterior\")\n",
        "ax2.set_title(\"Variational approx.\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEoeWbq7vMeJ"
      },
      "outputs": [],
      "source": [
        "def plot_decision_boundary(xx, yy, P, ax):\n",
        "    P = P.reshape(*xx.shape)\n",
        "    levels = [0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 1]\n",
        "    cs = ax.contour(xx, yy, P, levels, colors=\"k\", linewidths=1.8, zorder=100)\n",
        "    ax.clabel(cs, inline=1, fontsize=10)\n",
        "    cs = ax.contourf(xx, yy, P, levels, cmap=\"Purples_r\", alpha=0.5)\n",
        "\n",
        "\n",
        "xx, yy, Xt = get_grid((-7, 7), N=50)\n",
        "ps = # @@ COMPLETE @@ #\n",
        "\n",
        "fig, ax = plt.subplots(figsize=[5, 4])\n",
        "plot_decision_boundary(xx, yy, ps, ax=ax)\n",
        "plot_data(X, y, ax=ax)\n",
        "\n",
        "ax.set_xlabel(r\"$\\boldsymbol{x}_0$\")\n",
        "ax.set_ylabel(r\"$\\boldsymbol{x}_1$\")\n",
        "ax.set_title(\"Predictive density\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xaz9ZlhZvMeK"
      },
      "source": [
        "**Exercise:**\n",
        "At convergence, sample 1000 times the ELBO like we did before, and plot it with a boxplot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2goM5w5DvMeK"
      },
      "outputs": [],
      "source": [
        "converged_elbos = pd.DataFrame()\n",
        "n_repetition = 1000\n",
        "converged_elbos[\"Diagonal posterior\"] = # @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdS2ZKgovMeK"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=[4, 3])\n",
        "sns.boxplot(\n",
        "    data=converged_elbos,\n",
        "    whis=np.inf,\n",
        ")\n",
        "ax.axhline(-0.4, color=\"xkcd:red\", label=r\"$p(\\boldsymbol{y}|\\boldsymbol{X})$\")\n",
        "ax.set_title(\"ELBO at convergence\", y=1.02)\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSNwUN_kvMeL"
      },
      "source": [
        "# 6. Alternatives to Mean Field Variational Inference\n",
        "\n",
        "As we saw from one of the previous question, the approximation that we used is very rough: it recoves some properties of the true posterior but fails to capture the strong correlation that exists in the parameter space. \n",
        "What we need to do it to increase the complexity and the expressiveness of the variational posterior. \n",
        "The first step that we can do is to introduce a non-diagonal covariance $\\boldsymbol{w} \\sim \\mathcal{N}(\\mu, \\Sigma)$, where the covariance $\\Sigma=LL^\\top$ ($L$ is a lower triangular matrix).\n",
        "\n",
        "Sampling from such distribution is possible again using the reparameterization trick,\n",
        "\n",
        "\\begin{equation}\n",
        "\\tilde{\\boldsymbol{w}} = \\mu + \\mathrm{Tril}(L)\\boldsymbol{\\varepsilon} \\quad \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0, \\mathrm{I})\n",
        "\\end{equation}\n",
        "\n",
        "where $\\mathrm{Tril}(\\cdot)$ returns the lower triangular part of the matrix (the other elements are set to 0).\n",
        "\n",
        "**Exercise:** \n",
        "Complete the next functions to model a full covariance Gaussian distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SbCvL_1vMeL"
      },
      "outputs": [],
      "source": [
        "class GaussianFullCov(NamedTuple):\n",
        "    mean: jnp.array\n",
        "    L: jnp.array\n",
        "\n",
        "\n",
        "def sample_gaussian_fullcov(key, params):\n",
        "    mean, L = params\n",
        "    eps = # @@ COMPLETE @@ #\n",
        "    return # @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb7ZPPyHvMeL"
      },
      "outputs": [],
      "source": [
        "q_params = GaussianFullCov(jnp.zeros(2), jnp.eye(2))\n",
        "\n",
        "print(sample_gaussian_fullcov(jax.random.PRNGKey(1), q_params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdhTypk3vMeL"
      },
      "source": [
        "Now we need to compute the KL divergence between $q$ Gaussian with full covariance and $p$ Gaussian with diagonal covariance. \n",
        "Remember that the expression of the KL divergence between multivariate Gaussians $q = \\mathcal{N}(\\boldsymbol{\\mu}_q, \\Sigma_q)$ and $p = \\mathcal{N}(\\boldsymbol{\\mu}_p, \\Sigma_p)$ is as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathrm{KL}[q || p] = \n",
        "\\frac{1}{2} \\mathrm{Tr}(\\Sigma_p^{-1} \\Sigma_q)\n",
        "+ \\frac{1}{2} (\\mu_p - \\mu_q)^{\\top} \\Sigma_p^{-1} (\\mu_p - \\mu_q)\n",
        "- \\frac{D}{2} \n",
        "+ \\frac{1}{2} \\log\\left( \\frac{\\mathrm{det}\\Sigma_p}{\\mathrm{det}\\Sigma_q} \\right)\n",
        "\\end{equation}\n",
        "\n",
        "**Question:**\n",
        "Given that $q = \\mathcal{N}(\\boldsymbol{\\mu}_q, LL^\\top)$ and $p = \\mathcal{N}(\\boldsymbol{\\mu}_p, \\sigma^2_p\\mathrm{I})$, write the simplified KL divergence. \n",
        "\n",
        "*Hints:* \n",
        "\n",
        "- $\\mathrm{Tr}(\\boldsymbol L \\boldsymbol L^\\top) = \\sum_i\\mathrm{diag}(\\boldsymbol L)_i^2$ \n",
        "\n",
        "- $\\log\\mathrm{det}(\\boldsymbol L\\boldsymbol L^\\top) = \\sum_i\\log\\mathrm{diag}(\\boldsymbol L)_i^2$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9RoNO7QvMeM"
      },
      "outputs": [],
      "source": [
        "def kl_full_diag(q_params, p_params):\n",
        "    assert isinstance(q_params, GaussianFullCov)\n",
        "    assert isinstance(p_params, GaussianDiagonal)\n",
        "\n",
        "    mean_q, L_q = q_params\n",
        "    mean_p, log_var_p = p_params\n",
        "    kl = # @@ COMPLETE @@ #\n",
        "    return 0.5 * (kl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUguRtG5vMeM"
      },
      "outputs": [],
      "source": [
        "print(kl_full_diag(q_params, p_params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBepqJFTvMeM"
      },
      "source": [
        "**Exercise:**\n",
        "Create a new ELBO which uses this new distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yGttqmRvMeN"
      },
      "outputs": [],
      "source": [
        "elbo_fn = create_elbo_fn(\n",
        "    sample_fn=# @@ COMPLETE @@ #\n",
        "    kl_divergence_fn=# @@ COMPLETE @@ #\n",
        "    likelihood_fn=bernoulli_density,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D_seK-wvMeN"
      },
      "source": [
        "**Exercise:**\n",
        "Train this new model exactly the same as before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6qJGFievMeN"
      },
      "outputs": [],
      "source": [
        "# @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKDPl0PMvMeO"
      },
      "outputs": [],
      "source": [
        "# @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkdEh_g1vMeO"
      },
      "source": [
        "**Exercise:**\n",
        "Plot the train curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0RiJ0gNvMeO"
      },
      "outputs": [],
      "source": [
        "# @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o9P_UNxvMeP"
      },
      "source": [
        "**Exercise:**\n",
        "Plot the densities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPSkRCU_vMeP"
      },
      "outputs": [],
      "source": [
        "# @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoLXkhXdvMeP"
      },
      "source": [
        "**Exercise:**\n",
        "Plot the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAsEKADtvMeP"
      },
      "outputs": [],
      "source": [
        "# @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j0MP49tvMeQ"
      },
      "source": [
        "**Exercise:**\n",
        "Plot the ELBO at convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHjmaeNCvMeQ"
      },
      "outputs": [],
      "source": [
        "converged_elbos[\"Full cov. posterior\"] = # @@ COMPLETE @@ #\n",
        "# @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNUsv_jYvMeQ"
      },
      "source": [
        "**Question:**\n",
        "Do you observe something interesting? \n",
        "Remember what the ELBO represents. It is the lower bound of the marginal distribution $p(\\boldsymbol{y}|\\boldsymbol{X})$. Check the first lab on Bayesian linear regression if you don't remember what the marginal distribution measures. Based solely on this value, which model would you choose? Why?    "
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "rise": {
      "scroll": true,
      "theme": "sky"
    },
    "toc-autonumbering": false,
    "colab": {
      "name": "Variational_Logistic_Regression.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}