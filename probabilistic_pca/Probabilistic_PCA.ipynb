{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ddadfd28-a578-46b0-a541-0e81ee1a4f20",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "tags": [],
        "id": "ddadfd28-a578-46b0-a541-0e81ee1a4f20"
      },
      "source": [
        "## Dimensionality reduction, Latent Variable Models and (Probabilistic) Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c83d33f7-700a-4040-860e-dc291d9dd761",
      "metadata": {
        "id": "c83d33f7-700a-4040-860e-dc291d9dd761"
      },
      "source": [
        "In this lab, you will implement and analyze one classic dimensionality reduction method (PCA) and it's corresponding probabilistic interpretation. \n",
        "\n",
        "By the end of this lab, you will be able to \n",
        "- implement PCA from zero, using linear algebra\n",
        "- extend PCA to be probabilistic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede781ac-7940-4955-bd8a-cbc15ea8f012",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ede781ac-7940-4955-bd8a-cbc15ea8f012"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import rc\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "colab = \"google.colab\" in str(get_ipython())\n",
        "preamble = r\"\"\"\\renewcommand{\\familydefault}{\\sfdefault}\\usepackage{sansmath}\n",
        "\\usepackage{FiraSans}\\sansmath\\usepackage{amsmath}\\usepackage{bm}\"\"\"\n",
        "\n",
        "rc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": \"DejaVu Sans\"})\n",
        "rc(\"text\", **{\"usetex\": not colab, \"latex.preamble\": preamble})\n",
        "rc(\"figure\", **{\"dpi\": 200, \"figsize\": [5, 3]})\n",
        "rc(\n",
        "    \"axes\",\n",
        "    **{\"spines.right\": False, \"spines.top\": False, \"xmargin\": 0.0, \"ymargin\": 0.05}\n",
        ")\n",
        "\n",
        "plt.rcParams.update(\n",
        "    {\n",
        "        \"lines.color\": \"white\",\n",
        "        \"patch.edgecolor\": \"white\",\n",
        "        \"text.color\": \"white\",\n",
        "        \"axes.facecolor\": \"#21252B\",\n",
        "        \"axes.edgecolor\": \"white\",\n",
        "        \"axes.labelcolor\": \"white\",\n",
        "        \"xtick.color\": \"white\",\n",
        "        \"ytick.color\": \"white\",\n",
        "        \"grid.color\": \"white\",\n",
        "        \"figure.facecolor\": \"#21252B\",\n",
        "        \"figure.edgecolor\": \"#21252B\",\n",
        "        \"savefig.facecolor\": \"#21252B\",\n",
        "        \"savefig.edgecolor\": \"#21252B\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2707b3fc-3828-443b-a622-af236a6963c6",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "2707b3fc-3828-443b-a622-af236a6963c6"
      },
      "source": [
        "Let's start to generate the data that we will be using in the notebook (we will also have some labels just to simplify visualization)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a36afd83-beca-41d3-af13-d6927a384b81",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "a36afd83-beca-41d3-af13-d6927a384b81"
      },
      "outputs": [],
      "source": [
        "def generate_data(N):\n",
        "    rng = iter(jax.random.split(jax.random.PRNGKey(1), 10))\n",
        "    x0 = jax.random.uniform(next(rng), shape=(N // 2,))  # + 0.1\n",
        "    x1 = jax.random.uniform(next(rng), shape=(N // 2,)) * 4 - 2\n",
        "    x2 = 0.5 * jax.random.normal(next(rng), shape=(len(x0),))\n",
        "    x10 = jax.random.uniform(next(rng), shape=(N // 2,)) * -1  # - 0.1\n",
        "    x11 = jax.random.uniform(next(rng), shape=(N // 2,)) * 4 - 2\n",
        "    x12 = 0.5 * jax.random.normal(next(rng), shape=(len(x10),))\n",
        "\n",
        "    X = jnp.stack([x0, x10, x1, x11, x2, x12]).reshape(3, N).T\n",
        "    return X  # , jnp.concatenate([jnp.zeros(N // 2), jnp.ones(N // 2)])\n",
        "\n",
        "\n",
        "X = generate_data(250)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9efd18a5-51d4-4fef-9174-c942377eca71",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9efd18a5-51d4-4fef-9174-c942377eca71"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import IntSlider, interact\n",
        "\n",
        "\n",
        "def animate(angle):\n",
        "    fig = plt.figure(figsize=[5, 5])\n",
        "    config = dict(edgecolor=\"black\", linewidth=1, zorder=10)\n",
        "    ax = fig.add_subplot(projection=\"3d\")\n",
        "    ax.scatter(*X.T, facecolor=\"tab:red\", **config)\n",
        "    ax.view_init(angle, azim=angle)\n",
        "    ax.tick_params(axis=\"both\", which=\"major\", pad=1)\n",
        "    ax.set_xlabel(r\"$x_0$\")\n",
        "    ax.set_ylabel(r\"$x_1$\")\n",
        "    ax.set_zlabel(r\"$x_2$\")\n",
        "    ax.set_title(r\"Observed space $X$\")\n",
        "    ax.set_zlim(-3, 3)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "interact(animate, angle=IntSlider(min=0, max=360, step=1, value=20));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfad7509-4375-4064-b4ff-9e11782faa90",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "bfad7509-4375-4064-b4ff-9e11782faa90"
      },
      "source": [
        "### Review of PCA \n",
        "\n",
        "The most common derivation of PCA is in terms of a standardized linear projection which\n",
        "maximizes the variance in the projected space. For a set of observed $d$-dimensional data vectors \n",
        "$\\boldsymbol X = \\{\\boldsymbol x_n\\}_{n=1}^N$, the $q$ principal axes $\\boldsymbol w_j$, for $j=1,...,Q$ are\n",
        "those orthonormal axes onto which the retained variance under projection is maximal. It can\n",
        "be shown that the vectors $\\boldsymbol w_j$ are given by the $q$ dominant eigenvectors (i.e. those with the\n",
        "largest associated eigenvalues $\\lambda_j$) of the sample covariance matrix $\\boldsymbol S = \\frac 1N \\boldsymbol X^\\top\\boldsymbol X$, such that $\\boldsymbol S\\boldsymbol w_j = \\lambda_j \\boldsymbol w_j$. \n",
        "The $q$ principal components of the\n",
        "observed vector $\\boldsymbol x_n$ are given by the (latent) vector $\\boldsymbol z_n = \\boldsymbol W\\boldsymbol x_n$, where $\\boldsymbol W = [\\boldsymbol w_1, ..., \\boldsymbol w_q]^\\top$.\n",
        "Note, for sake of simplicity, we suppose to have centered data, such that the mean is zero. In practice, this is not a stric requirement (we can always remove the mean from the data)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95388324-b7d9-47ad-986f-34c0fe615296",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "95388324-b7d9-47ad-986f-34c0fe615296"
      },
      "source": [
        "**Exercise**: \n",
        "Complete the next function to compute the PCA matrix $\\boldsymbol W$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac3aa9b-875f-4b8d-b383-e71cb2d42959",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "6ac3aa9b-875f-4b8d-b383-e71cb2d42959"
      },
      "outputs": [],
      "source": [
        "def PCA(X, q=2):\n",
        "    # @@ COMPLETE @@ #\n",
        "    return W\n",
        "\n",
        "\n",
        "def project_latents_PCA(X, W):\n",
        "    # @@ COMPLETE @@ #\n",
        "    return Z"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96bd75f7-998b-4eb2-88ae-7ebdb927d5d5",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "96bd75f7-998b-4eb2-88ae-7ebdb927d5d5"
      },
      "source": [
        "**Exercise**:\n",
        "Compute the PCA of $\\boldsymbol X$ in 2 dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f92c5121-81e6-4529-8b84-91ebc4159111",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "f92c5121-81e6-4529-8b84-91ebc4159111"
      },
      "outputs": [],
      "source": [
        "W = # @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "662d19b2-527d-4ca5-8d00-5801aacde93a",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "662d19b2-527d-4ca5-8d00-5801aacde93a"
      },
      "source": [
        "**Exercise**: Compute the latent variable $\\boldsymbol Z$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "250f1e0e-6cac-4ecc-a040-6e8fa172cf06",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "250f1e0e-6cac-4ecc-a040-6e8fa172cf06"
      },
      "outputs": [],
      "source": [
        "Z = # @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84aace97-4730-4049-bfa3-85cb1ad883cc",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "84aace97-4730-4049-bfa3-85cb1ad883cc"
      },
      "source": [
        "**Exercise**:\n",
        "Plot the projection in the latent 2D space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "508d6864-411b-4dda-9b36-09583ae7da85",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "508d6864-411b-4dda-9b36-09583ae7da85"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=[4, 4])\n",
        "config = dict(edgecolor=\"black\", linewidth=1, zorder=10)\n",
        "ax.scatter(*Z.T, facecolor=\"tab:blue\", **config)\n",
        "ax.margins(0.05)\n",
        "ax.set_title(r\"Latent space $Z$\")\n",
        "ax.set_xlabel(r\"$x_0$\")\n",
        "ax.set_ylabel(r\"$x_1$\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12efc267-7c5b-46bd-a110-2296b32d612e",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "12efc267-7c5b-46bd-a110-2296b32d612e"
      },
      "source": [
        "## Probabilistic PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f72163d9-d29a-4c02-9671-49b612afad93",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "f72163d9-d29a-4c02-9671-49b612afad93"
      },
      "source": [
        "A notable feature of these definitions of PCA (and one remarked on in many\n",
        "texts) is the absence of an associated probabilistic model for the observed data. The objective\n",
        "of this section is to address this limitation by demonstrating that PCA may indeed be\n",
        "derived within a density estimation framework.\n",
        "Chapter 12 of Bishop's [**Pattern Recognition and Machine Learning**](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) is a good reference (and a suggested reading) for latent variable models.\n",
        "\n",
        "Generally, we can define our model in the following way:\n",
        "\n",
        "$$\n",
        "\\boldsymbol x_n = \\boldsymbol W \\boldsymbol z_n + \\boldsymbol \\varepsilon_n\n",
        "$$\n",
        "\n",
        "with $\\boldsymbol\\varepsilon_n\\sim\\mathcal N(\\boldsymbol 0, \\sigma^2\\boldsymbol I)$. \n",
        "This is also equivalent to define the following per-point likelihood: \n",
        "\n",
        "$$\n",
        "p(\\boldsymbol x_n|\\boldsymbol W,\\boldsymbol z_n) = \\mathcal{N}( \\boldsymbol W \\boldsymbol z_n, \\sigma^2\\boldsymbol I)\n",
        "$$\n",
        "\n",
        "<!-- Under the assumption of independence, we can then write: \n",
        "\n",
        "$$\n",
        "p(\\boldsymbol X|\\boldsymbol W,\\boldsymbol Z) = \\prod_{i=1}^N p(\\boldsymbol x_n|\\boldsymbol W,\\boldsymbol z_n)\n",
        "$$ -->\n",
        "\n",
        "If we want to treat this model probabilistically, there are several routes we can take \n",
        "1. Place a prior on $p(\\boldsymbol Z)$, and optimize the likelihood w.r.t. the values of $\\boldsymbol W$ after marginalization of $\\boldsymbol Z$ \n",
        "2. Place a prior on $p(\\boldsymbol W)$, and optimize the likelihood w.r.t. the values of $\\boldsymbol Z$ after marginalization of $\\boldsymbol W$ \n",
        "3. Place a prior on $p(\\boldsymbol Z)$ and $p(\\boldsymbol W)$ and compute the posterior  $p(\\boldsymbol Z, \\boldsymbol W|\\boldsymbol X)$ \n",
        "\n",
        "Of all the three approaches, (1) and (2) can have analytic solution, while (3) requires marginalizing both on $\\boldsymbol Z$ and $\\boldsymbol W$ which is generally intractable due to the non-conjugacy likelihood-prior."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7884a147-9bcd-476c-80e0-b8fbab0f1618",
      "metadata": {
        "id": "7884a147-9bcd-476c-80e0-b8fbab0f1618"
      },
      "source": [
        "### PPCA with EM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "373a8a80-e938-41b4-9e2d-283b44f2e5c6",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "373a8a80-e938-41b4-9e2d-283b44f2e5c6"
      },
      "source": [
        "In this lab, we will focus on approch (1).\n",
        "\n",
        "We define a Gaussian prior on $\\boldsymbol z_n$ as $p(\\boldsymbol z_n) = \\mathcal{N}(\\boldsymbol 0, \\boldsymbol I)$.\n",
        "While there exists a analitic solution, we will implement an iterative procedure based on Expectation-Maximization (EM).\n",
        "\n",
        "\n",
        "The objective is the following\n",
        "$$\n",
        "\\max_{\\boldsymbol W} p(\\boldsymbol X|\\boldsymbol W,\\sigma^2) = \\max_{\\boldsymbol W,\\sigma^2} \\int p(\\boldsymbol X, \\boldsymbol Z | \\boldsymbol W,\\sigma^2) \\text{d}\\boldsymbol Z\n",
        "$$\n",
        "The integral is often intractable, which motivates the use of an iterative method.\n",
        "Let's start by defining the joint (log)-likelihood: \n",
        "$$\n",
        "\\log p(\\boldsymbol X, \\boldsymbol Z | \\boldsymbol W,\\sigma^2) = \\sum_{n=1}^N \\log p(\\boldsymbol x_n, \\boldsymbol z_n) = \\sum_{n=1}^N \\left[\\log (\\boldsymbol x_n | \\boldsymbol z_n,\\sigma^2) + \\log p(\\boldsymbol z_n)\\right]\n",
        "$$\n",
        "\n",
        "\n",
        "In EM, we alternatively execute two steps:\n",
        "1. Expectation: \n",
        "$$\n",
        "f(\\boldsymbol \\theta;\\boldsymbol \\theta^{(t)}) = \\mathbb{E}_{p(\\boldsymbol Z | \\boldsymbol X, \\boldsymbol \\theta^{(t)})} \\log p(\\boldsymbol X, \\boldsymbol Z | \\boldsymbol \\theta)\n",
        "$$\n",
        "2. Maximization: \n",
        "$$\n",
        "\\boldsymbol \\theta^{(t+1)} = \\arg\\max_{\\boldsymbol \\theta}f(\\boldsymbol \\theta;\\boldsymbol \\theta^{(t)})\n",
        "$$\n",
        "\n",
        "where, for sake of notation, $\\boldsymbol \\theta = \\{\\boldsymbol W, \\sigma^2\\}$.\n",
        "\n",
        "For our problem both steps are analitical (and you encouraged to develop the expressions for expectation and for maximization).\n",
        "If you trust us, they simplify in the following update rule:\n",
        "\n",
        "$$\n",
        "\\boldsymbol W^{(t+1)} = \\boldsymbol S\\boldsymbol W (\\boldsymbol M^{-1}\\boldsymbol W^\\top\\boldsymbol S\\boldsymbol W)^{-1}\n",
        "$$\n",
        "$$\n",
        "\\sigma_n^{2^{(t+1)}} = \\frac 1 d \\text{Tr}(\\boldsymbol S - \\boldsymbol S\\boldsymbol W\\boldsymbol M^{-1}\\boldsymbol W^\\top )\n",
        "$$\n",
        "\n",
        "where $\\boldsymbol M =\\boldsymbol W^\\top\\boldsymbol W + \\sigma^2 I$ and $\\boldsymbol S = \\frac 1 N \\boldsymbol X^\\top\\boldsymbol X$ (here both $\\boldsymbol W$  and $\\sigma^2$ on the RHS are to be considered at iteration $t$)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22abd092-b6df-4a3e-a7b0-22ec79ac959f",
      "metadata": {
        "id": "22abd092-b6df-4a3e-a7b0-22ec79ac959f"
      },
      "source": [
        "**Question**:\n",
        "What is the dimensionality of $\\boldsymbol W$, $\\boldsymbol S$ and $\\boldsymbol M$? Can you derive the complexity of these two steps?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "467bdcec-5321-4f37-842c-37bbe70b3ec8",
      "metadata": {
        "id": "467bdcec-5321-4f37-842c-37bbe70b3ec8"
      },
      "source": [
        "**Exercise**: \n",
        "Implement PPCA using the EM algorithm above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f75a0e3-5812-4eb7-8238-6eac20d8540d",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0f75a0e3-5812-4eb7-8238-6eac20d8540d"
      },
      "outputs": [],
      "source": [
        "def PPCA_EM(X, W_init, s2_init, niter=100):\n",
        "    # @@ COMPLETE @@ #\n",
        "    W, s2 = W_init, s2_init\n",
        "    for _ in range(niter):\n",
        "        # @@ COMPLETE @@ #\n",
        "        \n",
        "    # @@ COMPLETE @@ #\n",
        "    return W, s2, Minv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ae8a410-7bd3-4834-bd15-5bb48380ea9d",
      "metadata": {
        "id": "0ae8a410-7bd3-4834-bd15-5bb48380ea9d"
      },
      "source": [
        "**Exercise**:\n",
        "Think about the initial value of W. Choose `W_init` to be zeros. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c11641c-1f4b-4a19-902f-a0e3ecf5112e",
      "metadata": {
        "id": "9c11641c-1f4b-4a19-902f-a0e3ecf5112e"
      },
      "outputs": [],
      "source": [
        "W_init = # @@ COMPLETE @@ #\n",
        "s2_init = 1\n",
        "W, s2, Minv = # @@ COMPLETE @@ #\n",
        "\n",
        "W"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "795f2849-8d7a-4788-bc16-41b23fa31009",
      "metadata": {
        "id": "795f2849-8d7a-4788-bc16-41b23fa31009"
      },
      "source": [
        "**Question**:\n",
        "You can surely initialize W_init randomly, but what is $\\boldsymbol W$ representing? What (linear algebra) properties of matrices is $\\boldsymbol W$ satisfying? (*Hint*: $\\boldsymbol W$ contains eigenvector). \n",
        "\n",
        "**Exercise**:\n",
        "Then, what kind of initial values are more suited for $\\boldsymbol W$? Try to implement it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9343ec9b-1f8c-46ba-9fcd-ec1ab655721b",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9343ec9b-1f8c-46ba-9fcd-ec1ab655721b"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "from scipy import stats\n",
        "\n",
        "W_init = # @@ COMPLETE @@ #\n",
        "s2_init = 1\n",
        "W, s2, Minv = # @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50bc4a68-139e-4a59-91f1-3dde4308420d",
      "metadata": {
        "id": "50bc4a68-139e-4a59-91f1-3dde4308420d"
      },
      "source": [
        "Now, we need to compute the projection in the latent space. \n",
        "Note that before this projection was purely deterministic, while now it's probabilistic, i.e. we are interested in $p(\\boldsymbol z|\\boldsymbol x)$.\n",
        "This quantity can be derived with Bayes' rule:\n",
        "$$\n",
        "p(\\boldsymbol z|\\boldsymbol x) = \\mathcal{N}(\\boldsymbol M^{-1} \\boldsymbol W \\boldsymbol x, \\sigma^2 \\boldsymbol M^{-1})\n",
        "$$\n",
        "\n",
        "**Exercise**: Complete the next function to project to the laten space and plot the results (use just the mean for the moment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d27b825-daf7-4a49-9a5c-3b409fa8d514",
      "metadata": {
        "id": "7d27b825-daf7-4a49-9a5c-3b409fa8d514"
      },
      "outputs": [],
      "source": [
        "def project_latents_PPCA(X, W, s2, Minv):\n",
        "    Zmean = # @@ COMPLETE @@ #\n",
        "    Zcov = # @@ COMPLETE @@ #\n",
        "    return Zmean, Zcov\n",
        "\n",
        "\n",
        "Zmean, Zcov = project_latents_PPCA(X, W, s2, Minv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60b0e71c-a0d2-43e3-82d9-5d8dcd16e424",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "60b0e71c-a0d2-43e3-82d9-5d8dcd16e424"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=[4, 4])\n",
        "config = dict(edgecolor=\"black\", linewidth=1, zorder=10)\n",
        "ax.scatter(*Zmean.T, facecolor=\"tab:blue\", **config)\n",
        "ax.margins(0.05)\n",
        "ax.set_title(r\"Latent space $Z$\")\n",
        "ax.set_xlabel(r\"$z_0$\")\n",
        "ax.set_ylabel(r\"$z_1$\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "493e2f76-c05c-4f14-966f-49baf938eaf2",
      "metadata": {
        "id": "493e2f76-c05c-4f14-966f-49baf938eaf2"
      },
      "source": [
        "**Exercise**:\n",
        "Take just one point and plot the covariance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c91ecce-40ca-42f3-b60f-77c9088f558c",
      "metadata": {
        "id": "0c91ecce-40ca-42f3-b60f-77c9088f558c"
      },
      "outputs": [],
      "source": [
        "def plot_cov(mu, Sigma, nstd=2, label=\"\", color=\"tab:blue\", ax=None):\n",
        "    from matplotlib.patches import Ellipse\n",
        "\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "\n",
        "    vals, vecs = np.linalg.eigh(Sigma)\n",
        "    order = vals.argsort()[::-1]\n",
        "    vals, vecs = vals[order], vecs[:, order]\n",
        "\n",
        "    theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n",
        "\n",
        "    # Width and height are \"full\" widths, not radius\n",
        "    width, height = 2 * nstd * np.sqrt(vals)\n",
        "    ellip = Ellipse(\n",
        "        xy=mu, width=width, height=height, angle=theta, fill=False, color=color, lw=\"2\"\n",
        "    )\n",
        "    config = dict(edgecolor=\"black\", linewidth=1, zorder=10)\n",
        "    ax.scatter(mu[0], mu[1], facecolor=\"tab:blue\", **config)\n",
        "    ax.add_artist(ellip)\n",
        "    return ellip\n",
        "\n",
        "\n",
        "# @@ COMPLETE @@ #\n",
        "plt.xlim(-2, 2)\n",
        "plt.ylim(-2, 2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2172a49a-2207-425a-a9b8-0890a5aa8fee",
      "metadata": {
        "id": "2172a49a-2207-425a-a9b8-0890a5aa8fee"
      },
      "source": [
        "**Question**:\n",
        "Is the assumption of likelihood with same variance realistic in practice? Can you think of a quick fix to relax this assumption?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42c6535e-2142-497e-8ccc-c19b623d01f2",
      "metadata": {
        "id": "42c6535e-2142-497e-8ccc-c19b623d01f2"
      },
      "source": [
        "A final point to note is that, at convergence, although the columns of $W$ will span the principal\n",
        "subspace, they need not be orthogonal since\n",
        "$$\n",
        "\\boldsymbol W^\\top\\boldsymbol W =\\boldsymbol R^\\top (\\boldsymbol \\Lambda - \\sigma^2 \\boldsymbol I) \\boldsymbol R\n",
        "$$\n",
        "which is not diagonal for $\\boldsymbol R \\neq \\boldsymbol I$. In common with other iterative PCA\n",
        "algorithms, there is an element of rotational ambiguity, which means that the solution is unique up to an \n",
        "arbitrary rotation matrix $\\boldsymbol R$.\n",
        "\n",
        "However, if required, the true principal axes\n",
        "may be determined by noting that the previous equation represents $\\boldsymbol R$ as an eigenvector decomposition of $\\boldsymbol W^\\top\\boldsymbol W$.\n",
        "\n",
        "**Exercise**:\n",
        "Implement a new version of the projection and take into account this additional rotation matrix $\\boldsymbol R$. Plot the new results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70ba3d0b-65c6-48b4-8319-33f7470a5e93",
      "metadata": {
        "id": "70ba3d0b-65c6-48b4-8319-33f7470a5e93"
      },
      "outputs": [],
      "source": [
        "def project_latents_PPCA_rot(X, W, s2, Minv):\n",
        "    R = # @@ COMPLETE @@ #\n",
        "    Zmean = # @@ COMPLETE @@ #\n",
        "    Zcov = # @@ COMPLETE @@ #\n",
        "    return Zmean, Zcov\n",
        "\n",
        "\n",
        "Zmean, Zcov = project_latents_PPCA_rot(X, W, s2, Minv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "931eee43-99d0-4c01-8e2e-a1b4512ebff2",
      "metadata": {
        "id": "931eee43-99d0-4c01-8e2e-a1b4512ebff2"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=[4, 4])\n",
        "config = dict(edgecolor=\"black\", linewidth=1, zorder=10)\n",
        "ax.scatter(*Zmean.T, facecolor=\"tab:blue\", **config)\n",
        "ax.margins(0.05)\n",
        "ax.set_title(r\"Latent space $Z$\")\n",
        "ax.set_xlabel(r\"$z_0$\")\n",
        "ax.set_ylabel(r\"$z_1$\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c795ceb-20d9-4621-8918-e9dc9ede57f2",
      "metadata": {
        "id": "6c795ceb-20d9-4621-8918-e9dc9ede57f2"
      },
      "source": [
        "## Projection of MNIST\n",
        "\n",
        "As a final exercise, try to project three digits ('0', '1' and '2') from the MNIST dataset in 2 dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73c70b0a-4cae-4ec3-a4ec-50543114ba64",
      "metadata": {
        "id": "73c70b0a-4cae-4ec3-a4ec-50543114ba64"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/srossi93/asi-labs/master/lab_week5-public/mnist012.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b24e32cd-a5c4-4f39-95b9-1b8d51b87898",
      "metadata": {
        "id": "b24e32cd-a5c4-4f39-95b9-1b8d51b87898"
      },
      "outputs": [],
      "source": [
        "data = np.load(\"mnist012.npz\")\n",
        "X, y = data[\"X\"], data[\"y\"]\n",
        "Ximg = X.reshape(-1, 28, 28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d27f173-fa8a-48b4-bcba-4033a1776d2c",
      "metadata": {
        "id": "6d27f173-fa8a-48b4-bcba-4033a1776d2c"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(3, 10, figsize=[10, 3])\n",
        "for i, digit in enumerate([0, 1, 2]):\n",
        "    for j, image_idx in enumerate(range(10)):\n",
        "        axs[i, j].imshow(Ximg[y == digit][image_idx], cmap=plt.cm.gray_r)\n",
        "        axs[i, j].axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea6f6f62-bb97-4f10-a743-9abcf69c9eff",
      "metadata": {
        "id": "ea6f6f62-bb97-4f10-a743-9abcf69c9eff"
      },
      "source": [
        "**Exercise**:\n",
        "Run PPCA on this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ddad172-c9ef-4834-a88a-9f21a86187c6",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3ddad172-c9ef-4834-a88a-9f21a86187c6"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "W_init = # @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e384f8d-d010-4d6f-87c1-466bff3be93c",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9e384f8d-d010-4d6f-87c1-466bff3be93c"
      },
      "outputs": [],
      "source": [
        "W, s2, Minv = # @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f659691-5138-494a-9b01-8e664f1ec9d1",
      "metadata": {
        "id": "0f659691-5138-494a-9b01-8e664f1ec9d1"
      },
      "source": [
        "**Exercise**:\n",
        "Compute the projections, corrected with the rotation matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9f14f35-caf4-470a-b8de-819537eb8d81",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "a9f14f35-caf4-470a-b8de-819537eb8d81"
      },
      "outputs": [],
      "source": [
        "Zmean, Zcov = # @@ COMPLETE @@ #"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d59a185b-abfa-4df8-a1a0-438cb76514cd",
      "metadata": {
        "id": "d59a185b-abfa-4df8-a1a0-438cb76514cd"
      },
      "source": [
        "**Exercise**:\n",
        "Plot the results using the function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9417bd62-c286-4db9-9508-a839e3762960",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9417bd62-c286-4db9-9508-a839e3762960"
      },
      "outputs": [],
      "source": [
        "from matplotlib import offsetbox\n",
        "\n",
        "config = dict(edgecolor=\"black\", linewidth=1, alpha=0.3)\n",
        "\n",
        "\n",
        "def plot_projection_mnist(Ximg, y, Z):\n",
        "    shown_images = np.array([[np.inf, np.inf]])  # just something big\n",
        "    for i in range(len(Z)):\n",
        "        dist = np.sum((Z[i] - shown_images) ** 2, 1)\n",
        "        if np.min(dist) < 0.0065:\n",
        "            ## don't show points that are too close\n",
        "            continue\n",
        "        shown_images = np.r_[shown_images, [Z[i]]]\n",
        "        imagebox = offsetbox.AnnotationBbox(\n",
        "            offsetbox.OffsetImage(Ximg[i], zoom=0.42, cmap=plt.cm.gray_r), Z[i]\n",
        "        )\n",
        "        ax.add_artist(imagebox)\n",
        "\n",
        "    # Plot also some points\n",
        "    idx = np.random.permutation(len(Z))[:1000]\n",
        "    ax.scatter(*Z[y == 0][idx].T, facecolor=\"tab:orange\", **config)\n",
        "    ax.scatter(*Z[y == 1][idx].T, facecolor=\"tab:green\", **config)\n",
        "    ax.scatter(*Z[y == 2][idx].T, facecolor=\"tab:blue\", **config)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=[5, 4.5])\n",
        "\n",
        "\n",
        "plot_projection_mnist(Ximg, y, Zmean)\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Probabilistic_PCA.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}